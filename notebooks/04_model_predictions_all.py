#%%
import os
if os.getcwd().endswith('notebooks'):
    os.chdir('..')

#%% import libraries
import numpy as np
import pandas as pd
import xarray as xr
from notebooks.models import * 
from collections import namedtuple
import pickle
from soil_diskin.utils import download_file
from soil_diskin.age_dist_utils import predict_fnew
from soil_diskin.data_wrangling import parse_he_data
from notebooks.constants import *
from scipy.integrate import solve_ivp
from  scipy.io import loadmat
from joblib import Parallel, delayed, parallel_backend
from tqdm import tqdm

#%% Load the site data
site_data = pd.read_csv('results/processed_balesdant_2018_all.csv')
turnover_14C = pd.read_csv('results/tropical_sites_14C_turnover_all.csv')
#%% Power-law model

# load the power-law parameters
power_law_params = pd.read_csv('results/powerlaw_model_optimization_results_all2.csv')

predictions = []
for i, row in power_law_params.iterrows():
    model = PowerLawDisKin(tau_0=row['tau_0'], tau_inf=row['tau_inf'])
    predictions.append(model.cdfA(site_data.loc[i, 'Duration_labeling']))
predictions = np.array(predictions)
# Save the model predictions
np.savetxt(f'results/04_model_predictions/power_law_{pd.Timestamp.now().date().strftime("%d-%m-%Y")}_all2.csv', predictions)

#%% Lognormal model

# read lognormal cdfs that were generated by the julia script - 04b_lognormal_predictions.jl
lognormal_cdfs = pd.read_csv('results/04b_lognormal_cdfs.csv')
ts = lognormal_cdfs.columns.astype(float).values
predictions = []
for i, row in site_data.iterrows():
    site_cdf = interp1d(ts, lognormal_cdfs.loc[i, :].values / turnover_14C.loc[i, 'turnover'])
    predictions.append(site_cdf(row['Duration_labeling']))
predictions = np.array(predictions)
# Save the model predictions
np.savetxt(f'results/04_model_predictions/lognormal_{pd.Timestamp.now().date().strftime("%d-%m-%Y")}.csv', predictions)

# Create the config object for the model based on parameters for Evergreen broadleaf foress in Test S5. 
# in [Xia et al. 2013](https://onlinelibrary.wiley.com/doi/full/10.1111/gcb.12172)

# We use only the 6 pools for litter and soil carbon. To calulcate u for the soil pools, we use the
# u values for the plant pools, and thre transfer coefficients from the plant pools to the litter pools
CABLE_config = namedtuple('Config', ['K','eta','A','u'])
CABLE_config.K = np.array([10., 0.95, 0.49, 2.19, 0.12, 0.0027])
CABLE_config.eta = 0.4 * np.ones(6)  
CABLE_config.u = np.array([0.249 * 0.69 + 0.551 * 0.6, 0.249 * 0.31 + 0.551 * 0.4, 0.2, 0, 0, 0])
CABLE_config.A = -np.diag(np.ones(6))  # Assuming 5 pools for simplicity
CABLE_config.A[3,0] = 0.45
CABLE_config.A[3,1] = 0.36
CABLE_config.A[3,2] = 0.24
CABLE_config.A[4,1] = 0.14
CABLE_config.A[4,2] = 0.28
CABLE_config.A[4,3] = 0.39
CABLE_config.A[5,3] = 0.006
CABLE_config.A[5,4] = 0.003

# Create the CABLE model instance
cable_model = CABLE(CABLE_config)
ages_cable = np.arange(0,100_000,0.1)
pA_cable = cable_model.pA_ss(ages_cable)

# Save the model predictions
with open(f'../results/model_predictions/CABLE_{pd.Timestamp.now().date().strftime("%d-%m-%Y")}.pkl','wb') as f:
    pickle.dump([ages_cable, pA_cable.squeeze()],f)

#%% Run the CLM4.5 model

# load soil depth mat file 
fn = 'data/CLM5_global_simulation/soildepth.mat'
mat = loadmat(fn)
zisoi = mat['zisoi'].squeeze()
zsoi = mat['zsoi'].squeeze()
dz = mat['dz'].squeeze()
dz_node = mat['dz_node'].squeeze()

# load gridded nc file with the inputs, initial values, and the environmental variables
global_da = xr.open_dataset('data/CLM5_global_simulation/global_demo_in.nc')
global_da = global_da.rename({'LON':'x','LAT':'y'})
def fix_lon(ds):
    ds['x'] = xr.where(ds['x']>=180,ds['x']-360,ds['x'])
    return ds.sortby('x')

global_da = fix_lon(global_da)
global_da = global_da.rio.write_crs("EPSG:4326", inplace=True)

# define model parameters
CLM_params = xr.open_dataset('data/CLM5_global_simulation/clm5_params.c171117.nc')
taus = np.array([CLM_params['tau_cwd'],CLM_params['tau_l1'],CLM_params['tau_l2_l3'],CLM_params['tau_l2_l3'],CLM_params['tau_s1'],CLM_params['tau_s2'],CLM_params['tau_s3']]).squeeze()
Gamma_soil = 1e-4 
F_soil = 0

# create global configuration parameters
config = ConfigParams(decomp_depth_efolding=0.5, taus=taus, Gamma_soil=Gamma_soil, F_soil=F_soil,
                      zsoi=zsoi, zisoi=zisoi, dz=dz, dz_node=dz_node, nlevels=10, npools=7)
global_data = GlobalData(global_da)

tmax = 10_000  # years

def CLM45_predict_site(row, tmax):
    """Predict the age distribution for a given site using the CLM4.5 model."""
    ldd = global_data.make_ldd(*row[['Latitude','Longitude']].values)
    # CLM_model = CLM5(config, ldd)
    # ts = np.logspace(-1, 4, 1000)  # time in years
    # labeled = solve_ivp(CLM_model._dX, (0, 10_000), np.zeros(70), t_eval=ts, method='LSODA')
    # age_CDF = labeled.y.sum(axis=0) / labeled.y.sum(axis=0)[-1]
    age_CDF = predict_fnew(CLM5, config, ldd, tmax)
    return age_CDF

# def CLM45_predict_site_with_progress(args):
#     """Wrapper function to handle progress tracking."""
#     i, row = args
#     result = CLM45_predict_site(row)
#     return i, result


# ldd = global_data.make_ldd(*row[['Latitude','Longitude']].values)
# CLM_model = CLM5(config, ldd)
# ldd_no_input = LocDependentData(w=ldd.w, t=ldd.t, o=ldd.o, n=ldd.n, sand=ldd.sand, I=xr.zeros_like(ldd.I), X0=ldd.X0)
# CLM_model_no_input = CLM5(config, ldd_no_input)
# ts = np.logspace(-1, 4, 1000)  # time in years
# labeled = solve_ivp(CLM_model._dX, (0, 10_000), np.zeros(70), t_eval=ts)#, method = 'LSODA')
# predict only for sites with missing data
predictions = pd.read_csv('results/04_model_predictions/CLM45_17-07-2025.csv')
missing_sites = list(predictions.index[predictions.isna().all(axis=1)])

# Run predictions with progress bar
print(f"Running CLM45 predictions for {len(missing_sites)} missing sites...")

# Option 1: Use joblib with verbose output for progress tracking
# with parallel_backend('loky', n_jobs=-1):
#     predictions = Parallel(verbose=1)(
#         delayed(CLM45_predict_site)(site_data.iloc[i], tmax) for i in missing_sites
#     )

with parallel_backend('loky', n_jobs=-1):
    predictions = Parallel(verbose=1)(
        delayed(CLM45_predict_site)(site_data.iloc[i], tmax) for i in range(len(site_data))
    )

#TODO: fix the tmax - increase to 30_000 or 100_000 years


# Option 2: Sequential execution with tqdm progress bar (uncomment to use)
# predictions = []
# for i, site_idx in enumerate(tqdm(missing_sites, desc="CLM45 predictions")):
#     result = CLM45_predict_site(site_data.iloc[site_idx])
#     predictions.append(result)

# Option 3: Parallel with tqdm using chunks (uncomment to use)
# from concurrent.futures import ProcessPoolExecutor, as_completed
# predictions = [None] * len(missing_sites)
# with ProcessPoolExecutor(max_workers=-1) as executor:
#     # Submit all tasks
#     future_to_idx = {
#         executor.submit(CLM45_predict_site, site_data.iloc[site_idx]): idx 
#         for idx, site_idx in enumerate(missing_sites)
#     }
#     
#     # Process completed tasks with progress bar
#     for future in tqdm(as_completed(future_to_idx), total=len(missing_sites), desc="CLM45 predictions"):
#         idx = future_to_idx[future]
#         predictions[idx] = future.result()

predictions = np.array(predictions)

predictions = pd.DataFrame(predictions)
predictions.columns = np.logspace(-1, tmax, 1000)  # time in years

fnew_predictions = np.array([interp1d(predictions.columns, predictions.iloc[1])(site_data.iloc[i]['Duration_labeling']) for i in range(len(site_data))])

# Save the model predictions
predictions.to_csv(f'results/04_model_predictions/CLM45_{pd.Timestamp.now().date().strftime("%d-%m-%Y")}.csv', index=False)
np.savetxt(f'results/04_model_predictions/CLM45_fnew_{pd.Timestamp.now().date().strftime("%d-%m-%Y")}.csv', fnew_predictions)
#%% Run the JSBACH model

# from https://pure.mpg.de/rest/items/item_3279802_26/component/file_3316522/content#page=107.51 and 
# https://gitlab.dkrz.de/icon/icon-model/-/blob/release-2024.10-public/externals/jsbach/src/carbon/mo_carbon_process.f90
a_i=np.array([0.72, 5.9, 0.28, 0.031]) # Eq. 6.28
a_h = 0.0016 # Eq. 6.34
b1 = 9.5e-2; b2 = -1.4e-3; gamma = -1.21; # Eq. 6.30 - T in C and P in m/yr
phi1 = -1.71; phi2 = 0.86; r = -0.306; # Eq. 6.31

# Download temperature and precipitation data from the JSBACH model
download_file("https://gcbo-opendata.s3.eu-west-2.amazonaws.com/trendyv12-gcb2023/JSBACH/S3/JSBACH_S3_tas.nc", "data/model_params/JSBACH", "JSBACH_S3_tas.nc")
download_file("https://gcbo-opendata.s3.eu-west-2.amazonaws.com/trendyv12-gcb2023/JSBACH/S3/JSBACH_S3_pr.nc", "data/model_params/JSBACH", "JSBACH_S3_pr.nc")
download_file("https://gcbo-opendata.s3.eu-west-2.amazonaws.com/trendyv12-gcb2023/JSBACH/S3/JSBACH_S3_npp.nc", "data/model_params/JSBACH", "JSBACH_S3_npp.nc")

# Load the JSBACH forcing data and calculate the monthly means
JSBACH_tas = xr.open_dataarray('data/model_params/JSBACH/JSBACH_S3_tas.nc').groupby("time.month").mean()
JSBACH_pr = xr.open_dataarray('data/model_params/JSBACH/JSBACH_S3_pr.nc').groupby("time.month").mean()
JSBACH_npp = xr.open_dataarray('data/model_params/JSBACH/JSBACH_S3_npp.nc').groupby("time.month").mean()

def interp_da(da):
    da = da.rio.write_crs("EPSG:4326")
    da = da.rio.write_nodata(np.nan)
    da = da.rio.interpolate_na()
    return da
JSBACH_tas = interp_da(JSBACH_tas)
JSBACH_pr = interp_da(JSBACH_pr)
JSBACH_npp = interp_da(JSBACH_npp)

JSBACH_config = namedtuple('Config', ['a_i', 'a_h', 'b1', 'b2', 'gamma', 'phi1', 'phi2', 'r'])
JSBACH_config.a_i = a_i
JSBACH_config.a_h = a_h
JSBACH_config.b1 = b1
JSBACH_config.b2 = b2
JSBACH_config.gamma = gamma
JSBACH_config.phi1 = phi1
JSBACH_config.phi2 = phi2
JSBACH_config.r = r

def get_env_params(row):
    """Get the environmental parameters for a given site."""
    I = JSBACH_npp.sel(longitude=row.loc['Longitude'], latitude=row.loc['Latitude'], method='nearest').values * SECS_PER_DAY * DAYS_PER_YEAR * 1000  # convert kgC/m2/s to gC/m2/yr
    T = JSBACH_tas.sel(longitude=row.loc['Longitude'], latitude=row.loc['Latitude'], method='nearest').values - T_MELT  # convert K to C
    P = JSBACH_pr.sel(longitude=row.loc['Longitude'], latitude=row.loc['Latitude'], method='nearest').values * SECS_PER_DAY * DAYS_PER_YEAR / 1000  # convert kg/m2/s to m/yr
    d = 4  # from WoodLitterSize in https://gitlab.dkrz.de/icon/icon-model/-/blob/release-2024.10-public/externals/jsbach/data/lctlib_nlct21.def it is 4  # Example diameter values in cm
    return namedtuple('EnvParams', ['I', 'T', 'P', 'd'])(I, T, P, d)


def JSBACH_predict_site(row, tmax):
    """Predict the age distribution for a given site using the JSBACH model."""
    env_params = get_env_params(row)
    age_CDF = predict_fnew(JSBACH, JSBACH_config, env_params, tmax)

    return age_CDF

tmax = 100_000  # maximum time in years


with parallel_backend('loky', n_jobs=-1):
    JSBACH_predictions = Parallel(verbose=1)(
        delayed(JSBACH_predict_site)(site_data.iloc[i], tmax) for i in range(len(site_data))
    )
# JSBACH_predictions = pd.DataFrame([JSBACH_predict_site(site_data.iloc[i], tmax) for i in range(len(site_data))])
JSBACH_predictions = pd.DataFrame(JSBACH_predictions)
JSBACH_predictions.columns = np.logspace(-1, np.log10(tmax), 1000)  # time in years
JSBACH_fnew_predictions = np.array([interp1d(JSBACH_predictions.columns, JSBACH_predictions.iloc[1])(site_data.iloc[i]['Duration_labeling']) for i in range(len(site_data))])
# Save the model predictions
JSBACH_predictions.to_csv(f'results/04_model_predictions/JSBACH_{pd.Timestamp.now().date().strftime("%d-%m-%Y")}.csv', index=False)
np.savetxt(f'results/04_model_predictions/JSBACH_fnew_{pd.Timestamp.now().date().strftime("%d-%m-%Y")}.csv', JSBACH_fnew_predictions)


#%% Create reduced complexity model predictions

model_names = {'CESM':'CESM1','IPSL':'IPSL-CM5A-LR','MRI':'MRI-ESM1'}
file_names = {'CESM':'areacella_fx_CESM1-BGC_1pctCO2_r0i0p0','IPSL':'areacella_fx_IPSL-CM5A-LR_historicalNat_r0i0p0','MRI':'areacella_fx_MRI-ESM1_esmControl_r0i0p0'}

tropical_site_params = []
for model in ['CESM','IPSL','MRI']:
    mod = parse_he_data(model=model, file_names=file_names)
    extrapolated =  xr.concat([mod.sel(parameter=p).rio.write_nodata(np.nan).rio.set_spatial_dims('lon','lat').rio.write_crs(4326).rio.interpolate_na() for p in mod.parameter],dim='parameter')

    tropical_site_params.append(xr.concat([extrapolated.sel(lat=site_data.iloc[i]['Latitude'],lon=site_data.iloc[i]['Longitude'],method='ffill') for i in range(site_data.shape[0])],dim='site'))

tropical_site_params = xr.concat(tropical_site_params,dim='model')
tropical_site_params['model'] = ['CESM','IPSL','MRI']

RCM_config = namedtuple('Config', ['model','tau_fac', 'rs_fac', 'correct'])
RCM_config.tau_fac = {'CESM': 3.7, 'IPSL': 14, 'MRI': 13} # from Sierra et al. 2018
RCM_config.rs_fac = {'CESM': 0.34, 'IPSL': 0.07, 'MRI': 0.34} # Sierra et al. 2018
RCM_config.correct = True
RCM_params = namedtuple('Params', ['params'])

# model_name = 'CESM'
# ReducedComplexModel
# RCM_config.model = model_name
def RCM_predict_site(model_name, i, RCM_config, RCM_params, site_data):
    """Predict the age distribution for a given site using the Reduced Complexity Model."""
    RCM_config.model = model_name
    RCM_params.params = tropical_site_params.sel(model=[model_name])[0, i, :].values
    RCM_model = ReducedComplexModel(RCM_config, RCM_params)
    return RCM_model.cdf(np.array([site_data.iloc[i]['Duration_labeling']]))
# RCM_params.params = tropical_site_params.sel(model=[model_name])[0, i, :].values
# RCM_model = ReducedComplexModel(RCM_config, RCM_params)
# RCM_model.cdf(np.array([site_data.iloc[i]['Duration_labeling']]))
RCM_predictions = []
for model in model_names.keys():
    prediction = np.array([RCM_predict_site(model, i, RCM_config, RCM_params, site_data) for i in range(len(site_data))])
    RCM_predictions.append(prediction)
RCM_predictions = pd.DataFrame(np.array(RCM_predictions).squeeze(), index = model_names.values()).T
# Save the model predictions
RCM_predictions.to_csv(f'results/04_model_predictions/RCM_{pd.Timestamp.now().date().strftime("%d-%m-%Y")}.csv', index=False)
#%% test against Fortran implementation
  
# distribute NPP
fract_npp_2_woodPool = 0.3 # fraction of NPP that goes to wood pool
fract_npp_2_reservePool = 0.05 # fraction of NPP that goes to reserve pool
fract_npp_2_exudates = 0.05 # fraction of NPP that goes to root exudates
# NPP_2_woodPool    = fract_npp_2_woodPool * NPP # NPP mol(C)/m2/yr
# NPP_2_reservePool = fract_npp_2_reservePool * NPP
# NPP_2_rootExudates= fract_npp_2_exudates * NPP
# NPP_2_greenPool = (1 - fract_npp_2_woodPool - fract_npp_2_reservePool - fract_npp_2_exudates) * NPP


LeafLit_coef = np.array([0.4651, 0.304, 0.0942, 0.1367, 0.]) #coefficient to distribute leaf litter into 5 classes of chemical composition
WoodLit_coef = np.array([0.65, 0.025, 0.025, 0.3, 0.]) #coefficient to distribute woody litter into 5 classes of chemical composition

fract_wood_aboveGround = 0.7 # !< Fraction of C above ground in wood pool (for separation of woody litter into above and below ground litter pools)
fract_green_aboveGround = 0.5 # !< Fraction of C above ground in green pool (for separation of green litter into above and below ground litter pools)

NPP_fractions = np.array([1. - (fract_npp_2_woodPool + fract_npp_2_exudates), fract_npp_2_woodPool, fract_npp_2_exudates])
above_below = np.array([[fract_green_aboveGround, 1 - fract_green_aboveGround],
                        [fract_wood_aboveGround, 1 - fract_wood_aboveGround],
                        [0, 1]])
litter_pool_split = np.stack([LeafLit_coef,WoodLit_coef,np.array([0,1,0,0,0])])
#NPP * NPP_fractions (3,1 - green, wood, exudates) * litter_fractions (4,3)  * above_below (3,2) 
# ((NPP_fractions * litter_pool_split[:,:-1].T) @ above_below).T.flatten()
NPP_to_litter_pools = (NPP_fractions * litter_pool_split[:,:-1].T) # size 4,3

NPP_pools_above_below = np.stack([NPP_to_litter_pools,NPP_to_litter_pools],axis=2) * above_below # size 4,3,2
veg_to_litter = np.array([[1, 0, 1], [0, 1, 0]]) # the vegetation pools that contribute to the non-woody and woody litter pools
B_temp = (NPP_pools_above_below.transpose(0,2,1) @ veg_to_litter.T) # size 4,2,2
B = np.concatenate([B_temp.transpose(1,0,2).reshape(8,2),  np.zeros((1,2))]).T.flatten() # size 18,

one_vec = np.ones(12) # 12 pools in the JSBACH model
JSBACH_env_params = namedtuple('EnvParams', ['I', 'T', 'P', 'd'])(one_vec * DAYS_PER_YEAR, 25 * one_vec, one_vec, 4)

JSBACH_model = JSBACH(config=JSBACH_config,
                 env_params=JSBACH_env_params)

JSBACH_output = JSBACH_model._dX(t = 0, X = np.ones(18))[:9]
fortran_output = pd.read_csv('../../jsbach/yasso_output.csv')

assert np.allclose(fortran_output['Value'].values[:9],JSBACH_output[:9] * (1/DAYS_PER_YEAR) + np.ones(9), rtol=1e-3, atol=1e-3), "JSBACH model output does not match Fortran implementation output"
#%%


    # sole the ode
    term = ODETerm(ode)
    solver = Dopri5()
    t_max = 30_000
    # ts = jnp.concatenate([jnp.arange(0,1.01,0.01),jnp.arange(2,t_max,1)])
    CLM5_ts = jnp.logspace(-2,jnp.log10(t_max),1000)
    solution = diffeqsolve(term, solver, t0=0, t1=t_max, dt0=0.1, y0=u,max_steps=1000000,saveat=SaveAt(ts = CLM5_ts))

    ys = solution.ys.sum(axis=1)
    CLM5_site_cum_age_dist.append(cumtrapz(ys,x=CLM5_ts)/trapz(ys,x=CLM5_ts))

CLM5_site_cum_age_dist = np.stack(CLM5_site_cum_age_dist)

# %%
